---
title: "p8105_hw2_dl3757"
author: "Dang Lin dl3757"
date: "2024-10-02"
output:
 pdf_document: default
---

```{r, message = FALSE}
# Import the libraries
library(tidyverse)
library(readxl)
```

# Problem 1
```{r, message = FALSE}
# Read the csv file and clean the dataset
nyc_transit <- read_csv("./NYC_Transit_Subway_Entrance_And_Exit_Data.csv", 
                        na = c("NA", ".", "")) %>% 
  janitor::clean_names() %>% 
  select(line, station_name, station_latitude, 
         station_longitude, route1:route11, entry, vending, 
         entrance_type, ada) %>% 
  mutate(across(starts_with("route"), as.character)) %>%
  mutate(entry = case_match(entry, "YES" ~ TRUE, "NO" ~ FALSE))
```

Firstly, we read the CSV file and applied the janitor::clean_names() function to make all the variable names tidy and consistent in format. Next, we used the select() function to choose the necessary variables from the original dataset and rearranged the position of each column. After that, we converted all the route columns to character variables. Finally, we transformed the "entry" variable from a character to a logical variable using the case_match() function. After these data-cleaning steps, the dataset contains 1,868 observations and 19 variables, meaning the cleaned dataset has 1,868 rows and 19 columns. For instance, variables such as "station_name," "line," "station_latitude," "station_longitude," and different routes are included. This dataset is not entirely tidy because it contains a lot of missing values, which we may need to address by removing some of them depending on the analysis. Additionally, some route variables were not initially in the correct format, so converting them to character variables was necessary before performing further analysis.

```{r}
# The number of distinct stations
distinct_station <- nyc_transit %>% 
  distinct(station_name, line)

nrow(distinct_station)
```

There are 465 distinct stations.

```{r}
# The number of distinct ADA-compliant stations
ada_compliant_station <- nyc_transit %>% 
  filter(ada == TRUE) %>% 
  distinct(station_name, line)

nrow(ada_compliant_station)
```

There are 84 ADA-compliant stations.

```{r}
# The proportion of stations entrances/exits without vending allow entrance
nyc_transit %>%  
  filter(vending == "NO") %>%  
  pull(entry) %>%  
  mean()
```

The proportion of station entrances/exits without vending allow entrance is 0.3770492.

```{r}
# The number of distinct stations serve the A train
distinct_A <- nyc_transit %>% 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") %>% 
  filter(route == "A") %>% 
  distinct(station_name, line)

nrow(distinct_A)
```

Sixty distinct stations serve the A train.

```{r}
# The number of distinct stations that are ADA-compliant serve the A train
distinct_A_ada <- nyc_transit %>%  
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") %>%  
  filter(route == "A", ada == TRUE) %>%  
  distinct(station_name, line)

nrow(distinct_A_ada)
```

Of these 60 distinct stations that serve the A train, 17 stations are ADA-compliant.

# Problem 2
```{r}
# Load and clean the dataset "Mr. Trash Wheel"
mr_trash_wheel <- read_excel("./202409 Trash Wheel Collection Data.xlsx", 
                          sheet = "Mr. Trash Wheel", range = "A2:N655") %>% 
  janitor::clean_names() %>% 
  drop_na(dumpster) %>% 
  mutate(sports_balls = as.integer(round(sports_balls, 0))) %>% 
  mutate(year = as.character(year)) %>% 
  mutate(sheet = "Mr.")
```

```{r}
# Load and clean the dataset "Professor Trash Wheel"
professor_trash_wheel <- read_excel("./202409 Trash Wheel Collection Data.xlsx", 
                          sheet = "Professor Trash Wheel", range = "A2:M123") %>% 
  janitor::clean_names() %>% 
  drop_na(dumpster) %>% 
  mutate(year = as.character(year)) %>% 
  mutate(sheet = "Professor")
```

```{r}
# Load and clean the dataset "Gwynnda Trash Wheel"
gwynnda_trash_wheel <- read_excel("./202409 Trash Wheel Collection Data.xlsx", 
                          sheet = "Gwynnda Trash Wheel", range = "A2:L266") %>% 
  janitor::clean_names() %>% 
  drop_na(dumpster) %>% 
  mutate(year = as.character(year)) %>% 
  mutate(sheet = "Gwynnda")
```

```{r}
# Produce a single tidy dataset by merging different datasets
tidy_dataset = 
  bind_rows(mr_trash_wheel, professor_trash_wheel, 
            gwynnda_trash_wheel) %>%
  janitor::clean_names() %>% 
  select(sheet, everything())
```

After a series of data cleaning steps, the Mr. Trash Wheel dataset contains 651 observations and 15 variables, the Professor Trash Wheel dataset incorporates 119 observations and 14 variables, and the Gwynnda Trash Wheel dataset includes 263 observations and 13 variables. It is worth mentioning that most of the values in the "Wrappers" column of the Gwynnda Trash Wheel dataset are listed as "NA". However, we did not omit the rows without values for "Wrappers" because doing so would result in the loss of many informative observations. Subsequently, we combined the three datasets into one to perform a more comprehensive analysis. The resulting tidy dataset contains 1033 observations and 15 variables, with each observation labeled according to its source sheet. This combined dataset includes important variables such as "weight_tons", "volume_cubic_yards", "plastic_bottles", and others, with data collected between 2014 and 2024. The total weight of trash collected by Professor Trash Wheel is `r sum(tidy_dataset$weight_tons[tidy_dataset$sheet == "Professor"], na.rm = TRUE)` based on available data. Moreover, the total number of cigarette butts collected by Gwynnda in June of 2022 is `r sum(tidy_dataset$cigarette_butts[tidy_dataset$sheet == "Gwynnda" & format(tidy_dataset$date, "%Y-%m") == "2022-06"])`.

# Problem 3
```{r message = FALSE}
# Load and clean the dataset
bakers <- read_csv("/Users/apple/Desktop/P8105_hw2_dl3757/gbb_datasets/bakers.csv") %>%
  janitor::clean_names() %>% 
  mutate(baker = sub(" .*", "", baker_name)) %>% 
  select(baker, everything(), -baker_name) %>% 
  arrange(baker)

bakes <- read_csv("/Users/apple/Desktop/P8105_hw2_dl3757/gbb_datasets/bakes.csv") %>%
  janitor::clean_names() %>% 
  select(baker, series, everything()) %>% 
  arrange(baker)

results <- read_csv("/Users/apple/Desktop/P8105_hw2_dl3757/gbb_datasets/results.csv", 
                    skip = 2) %>%
  janitor::clean_names() %>% 
  arrange(baker)
```

```{r}
# Check for completeness and correctness
bakers_bakes <- anti_join(bakers, bakes, by = "baker")

bakes_results <- anti_join(bakes, results, by = c("series", "episode"))
```

```{r, message = FALSE}
# Produce a single tidy dataset by merging different datasets
baker_tidy_1 <- left_join(bakes, bakers) %>% 
  janitor::clean_names() %>% 
  select(baker, series, episode, everything())

baker_tidy_2 <- right_join(bakers, results) %>% 
  janitor::clean_names() %>% 
  select(baker, series, episode, result, everything()) %>% 
  arrange(baker)

baker_tidy_3 <- right_join(baker_tidy_1, baker_tidy_2) %>% 
  janitor::clean_names() %>% 
  select(baker, series, episode, result, everything()) %>% 
  arrange(baker)

# Export the CSV
write_csv(baker_tidy_3, 
          "/Users/apple/Desktop/P8105_hw2_dl3757/gbb_datasets/bake_tidy_data.csv")
```

Firstly, the three datasets—"bakers," "bakes," and "results"—were imported into R Studio using the read_csv() function. Then, the janitor::clean_names() function was applied to convert the variable names to a tidy and consistent format. For the "baker_name" column in the "baker" dataset, we created a new variable called "baker" that contains only the first names of the bakers, making it easier to merge the datasets in later steps. After creating the new variable, we used the select() function to rearrange the column positions and the arrange() function to order the rows by the bakers' names. Next, we checked the completeness and correctness of the datasets but chose not to remove any potentially overlapping rows and unmatched records. Subsequently, we used the left_join() and right_join() functions to combine the three datasets into a single tidy dataset. Finally, we saved this newly created merged dataset as a CSV file. The final dataset contains 1,136 observations and 10 variables, including "baker," "series," "episode," "results," and other relevant information for each baker. It is worth noting that the final dataset contains many missing values. However, we chose not to remove these values, as most of them will not affect our analysis.

```{r}
# Create a table showing the star baker or winner of each episode in Seasons 5 through 10
star_baker <- baker_tidy_3 %>% 
  janitor::clean_names() %>%
  filter(result == c("STAR BAKER"), 
         series >= 5 & series <= 10) %>%
  arrange(series, episode) %>% 
  select(series, episode, baker, result)

winner <- baker_tidy_3 %>% 
  janitor::clean_names() %>%
  filter(result == c("WINNER"), 
         series >= 5 & series <= 10) %>%
  arrange(series, episode) %>% 
  select(series, episode, baker, result)

table <- bind_rows(winner, star_baker)

knitr::kable(table, caption = "Winner and Star Bakers")
```

If a baker frequently earns STAR BAKER across episodes within a specific series, they are more likely to become the final WINNER of that series. From this table, we can observe that Nadiya and Rahul were predictable winners because they consistently achieved STAR BAKER during their series. However, despite Richard, Ian, and Steph earning STAR BAKER the most times in their respective series, they unfortunately did not win the final title.

```{r, message = FALSE}
# Import the viewership data
viewers <- read_csv("/Users/apple/Desktop/P8105_hw2_dl3757/gbb_datasets/viewers.csv") %>% 
  janitor::clean_names()

# View the first 10 rows of the dataset
viewers_10 <- head(viewers, 10)
knitr::kable(viewers_10) 
```

```{r}
# Calculate the average viewership for Season 1 and 5
viewership_avg <- viewers %>%
  summarise(`Average Viewership Season 1` = mean(series_1, na.rm = TRUE), 
            `Average Viewership Season 5` = mean(series_5, na.rm = TRUE))

knitr::kable(viewership_avg, 
             caption = "Average Viewership in Season 1 and 5")
```

The average viewership in Season 1 is 2.77, and the average viewership in Season 5 is 10.0393.
